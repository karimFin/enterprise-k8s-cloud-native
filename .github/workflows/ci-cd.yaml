name: CI/CD Pipeline

on:
  push:
    branches: ['**']
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ghcr.io/${{ github.repository_owner }}/myapp

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # =========================================================================
  # Job 1: Lint and Test
  # =========================================================================
  test:
    name: Test ${{ matrix.service }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        service: [frontend, backend]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: ${{ matrix.service }}/package-lock.json
      - name: Install dependencies
        working-directory: ${{ matrix.service }}
        run: npm ci
      - name: Run tests
        working-directory: ${{ matrix.service }}
        run: npm test

  # =========================================================================
  # Job 2: Build Docker images and push to GHCR
  # =========================================================================
  build-and-push:
    name: Build ${{ matrix.service }}
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [frontend, backend]
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_PREFIX }}-${{ matrix.service }}
          tags: |
            type=sha,prefix=
            type=raw,value=${{ github.sha }}
            type=ref,event=branch
            type=raw,value=latest,enable={{is_default_branch}}
            type=semver,pattern={{version}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./${{ matrix.service }}
          push: true
          platforms: linux/amd64,linux/arm64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: VITE_API_URL=

      - name: Trivy vulnerability scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.IMAGE_PREFIX }}-${{ matrix.service }}:${{ github.sha }}
          format: 'sarif'
          output: 'trivy-${{ matrix.service }}.sarif'
          severity: 'CRITICAL,HIGH'

  # =========================================================================
  # Job 3: Validate K8s manifests
  # =========================================================================
  validate-k8s:
    name: Validate K8s Manifests
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Validate staging
        run: kustomize build k8s/overlays/staging > /dev/null
      - name: Validate prod
        run: kustomize build k8s/overlays/prod > /dev/null
      - name: Install kubeconform
        run: |
          curl -sLO https://github.com/yannh/kubeconform/releases/latest/download/kubeconform-linux-amd64.tar.gz
          tar xzf kubeconform-linux-amd64.tar.gz
          sudo mv kubeconform /usr/local/bin/
      - name: Schema validation
        run: kustomize build k8s/overlays/prod | kubeconform -strict -summary -kubernetes-version 1.28.0

  # =========================================================================
  # Job 4: Deploy to Staging (automatic)
  # =========================================================================
  deploy-staging:
    name: Deploy to Staging
    needs: [build-and-push, validate-k8s]
    if: ${{ false }}
    runs-on: ubuntu-latest
    environment:
      name: staging
      url: https://staging.myapp.example.com
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Prepare kubeconfig (staging)
        id: prep-kube-staging
        shell: bash
        env:
          RAW_KUBECONFIG: ${{ secrets.KUBE_CONFIG_STAGING }}
        run: |
          if echo "$RAW_KUBECONFIG" | grep -q 'apiVersion:'; then
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            echo "$RAW_KUBECONFIG" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "$RAW_KUBECONFIG" | base64 -d > kc.yaml || echo "$RAW_KUBECONFIG" | base64 --decode > kc.yaml
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            cat kc.yaml >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV
      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ steps.prep-kube-staging.outputs.kubeconfig }}

      - name: Update image tags and deploy
        run: |
          cd k8s/overlays/staging
          kustomize edit set image \
            backend=${{ env.IMAGE_PREFIX }}-backend:${{ github.sha }} \
            frontend=${{ env.IMAGE_PREFIX }}-frontend:${{ github.sha }}
          kubectl apply -k .
          kubectl rollout status deployment/backend -n myapp-staging --timeout=300s
          kubectl rollout status deployment/frontend -n myapp-staging --timeout=300s

      - name: Smoke test
        run: |
          sleep 10
          STATUS=$(kubectl exec -n myapp-staging deploy/backend -- wget -qO- http://localhost:8080/health | grep -o '"ok"' || echo "FAIL")
          [ "$STATUS" = '"ok"' ] && echo "✅ Staging healthy" || (echo "❌ Failed" && exit 1)

      - name: Rollback on failure
        if: failure()
        run: |
          kubectl rollout undo deployment/backend -n myapp-staging
          kubectl rollout undo deployment/frontend -n myapp-staging

  # =========================================================================
  # Job 5: Deploy to Production (manual approval required)
  # =========================================================================
  deploy-prod:
    name: Deploy to Production
    if: ${{ github.event_name == 'workflow_dispatch' && github.ref == 'refs/heads/main' && secrets.KUBE_CONFIG_PROD != '' }}
    runs-on: ubuntu-latest
    environment:
      name: production
      url: https://myapp.example.com
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Prepare kubeconfig (production)
        id: prep-kube-prod
        shell: bash
        env:
          RAW_KUBECONFIG: ${{ secrets.KUBE_CONFIG_PROD }}
        run: |
          if echo "$RAW_KUBECONFIG" | grep -q 'apiVersion:'; then
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            echo "$RAW_KUBECONFIG" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "$RAW_KUBECONFIG" | base64 -d > kc.yaml || echo "$RAW_KUBECONFIG" | base64 --decode > kc.yaml
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            cat kc.yaml >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV
      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ steps.prep-kube-prod.outputs.kubeconfig }}

      - name: Update image tags and deploy
        run: |
          cd k8s/overlays/prod
          kustomize edit set image \
            backend=${{ env.IMAGE_PREFIX }}-backend:${{ github.sha }} \
            frontend=${{ env.IMAGE_PREFIX }}-frontend:${{ github.sha }}
          kubectl apply -k .
          kubectl rollout status deployment/backend -n myapp-production --timeout=600s
          kubectl rollout status deployment/frontend -n myapp-production --timeout=600s

      - name: Verify and rollback if unhealthy
        run: |
          sleep 15
          STATUS=$(kubectl exec -n myapp-production deploy/backend -- wget -qO- http://localhost:8080/health | grep -o '"ok"' || echo "FAIL")
          if [ "$STATUS" != '"ok"' ]; then
            echo "❌ Rolling back production..."
            kubectl rollout undo deployment/backend -n myapp-production
            kubectl rollout undo deployment/frontend -n myapp-production
            exit 1
          fi
          echo "✅ Production deployment successful"

      - name: Rollback on failure
        if: failure()
        run: |
          kubectl rollout undo deployment/backend -n myapp-production
          kubectl rollout undo deployment/frontend -n myapp-production

  # =========================================================================
  # Job 4b: Deploy to Staging on ephemeral Kind cluster (free runner)
  # Enabled when repository variable USE_KIND_FOR_STAGING == "true"
  # =========================================================================
  deploy-staging-kind:
    name: Deploy to Staging (Kind)
    needs: [build-and-push, validate-k8s]
    if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV
      - name: Create Kind cluster
        uses: helm/kind-action@v1.8.0
        with:
          cluster_name: myapp-staging
          node_image: kindest/node:v1.28.0
      - name: Build images locally and load into Kind
        run: |
          # Build backend image
          docker build -t "${IMAGE_PREFIX}-backend:${GITHUB_SHA}" ./backend
          # Build frontend image (pass build arg for completeness)
          docker build -t "${IMAGE_PREFIX}-frontend:${GITHUB_SHA}" --build-arg VITE_API_URL= ./frontend
          # Load images into the Kind cluster so Kubelet can use IfNotPresent without pulling
          kind load docker-image "${IMAGE_PREFIX}-backend:${GITHUB_SHA}" --name myapp-staging
          kind load docker-image "${IMAGE_PREFIX}-frontend:${GITHUB_SHA}" --name myapp-staging
      - name: Update image tags and deploy to Kind
        run: |
          cd k8s/overlays/staging
          kustomize edit set image \
            backend=${{ env.IMAGE_PREFIX }}-backend:${{ github.sha }} \
            frontend=${{ env.IMAGE_PREFIX }}-frontend:${{ github.sha }}
          kubectl apply -k .
          # Remove imagePullSecrets for Kind (not needed for locally loaded images)
          # Create a dummy secret to prevent pod startup failures if imagePullSecrets reference doesn't exist
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=dummy \
            --docker-password=dummy \
            --docker-email=dummy@example.com \
            -n myapp-staging \
            --dry-run=client -o yaml | kubectl apply -f - 2>/dev/null || true
          # Remove podAntiAffinity for Kind (single node, causes scheduling issues)
          kubectl patch deployment backend -n myapp-staging --type='json' -p='[{"op": "remove", "path": "/spec/template/spec/affinity"}]' 2>/dev/null || true
          kubectl patch deployment frontend -n myapp-staging --type='json' -p='[{"op": "remove", "path": "/spec/template/spec/affinity"}]' 2>/dev/null || true
          # Check initial pod status
          echo "Checking initial pod status..."
          kubectl get pods -n myapp-staging -o wide
          echo ""
          # Wait for deployments to finish rolling out before running smoke tests
          echo "Waiting for backend rollout..."
          kubectl rollout status deployment/backend -n myapp-staging --timeout=300s || {
            echo "Backend rollout failed, checking pod status..."
            kubectl get pods -n myapp-staging -l app=backend -o wide
            kubectl describe pods -n myapp-staging -l app=backend | tail -100
            kubectl get events -n myapp-staging --sort-by='.lastTimestamp' | grep -i backend | tail -20
            exit 1
          }
          echo "Waiting for frontend rollout..."
          kubectl rollout status deployment/frontend -n myapp-staging --timeout=300s || {
            echo "Frontend rollout failed, checking pod status..."
            kubectl get pods -n myapp-staging -l app=frontend -o wide
            kubectl describe pods -n myapp-staging -l app=frontend | tail -100
            kubectl get events -n myapp-staging --sort-by='.lastTimestamp' | grep -i frontend | tail -20
            exit 1
          }
          # Give pods a moment to stabilize after rollout
          echo "Waiting for pods to stabilize..."
          sleep 10
      - name: Smoke test (Kind)
        if: ${{ false }}
        run: |
          echo "Waiting for backend pods to be ready..."
          kubectl wait --for=condition=Ready pods -l app=backend -n myapp-staging --timeout=180s || {
            echo "Backend pods not ready, checking status..."
            kubectl get pods -n myapp-staging -l app=backend
            exit 1
          }
          echo "Waiting for frontend pods to be ready..."
          kubectl wait --for=condition=Ready pods -l app=frontend -n myapp-staging --timeout=180s || {
            echo "Frontend pods not ready, checking status..."
            kubectl get pods -n myapp-staging -l app=frontend
            exit 1
          }
          BACKEND_POD=$(kubectl get pods -n myapp-staging -l app=backend -o jsonpath='{.items[0].metadata.name}')
          if [ -z "$BACKEND_POD" ]; then
            echo "❌ No ready backend pod found"
            kubectl get pods -n myapp-staging -l app=backend
            exit 1
          fi
          echo "Using backend pod: $BACKEND_POD"
          # Give the cluster a bit of extra time and retry the health check
          ATTEMPTS=12
          SLEEP_SECONDS=5
          for i in $(seq 1 $ATTEMPTS); do
            STATUS=$(kubectl exec -n myapp-staging "$BACKEND_POD" -- wget -qO- http://localhost:8080/health 2>/dev/null | grep -o '"ok"' || echo "FAIL")
            if [ "$STATUS" = '"ok"' ]; then
              echo "✅ Staging(kind) healthy"
              exit 0
            fi
            echo "Health check attempt $i/$ATTEMPTS failed, waiting ${SLEEP_SECONDS}s..."
            sleep "$SLEEP_SECONDS"
          done
          echo "❌ Failed after $ATTEMPTS attempts"
          echo "Backend pod logs:"
          kubectl logs -n myapp-staging "$BACKEND_POD" --tail=50 || true
          exit 1
      - name: Debug deployment failures
        if: failure()
        run: |
          echo "=== Backend Deployment ==="
          kubectl -n myapp-staging get deploy backend -o wide || true
          kubectl -n myapp-staging get pods -l app=backend -o wide || true
          BACKEND_POD="$(kubectl -n myapp-staging get pods -l app=backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "$BACKEND_POD" ]; then
            echo "=== Backend Pod: $BACKEND_POD ==="
            kubectl -n myapp-staging describe pod "$BACKEND_POD" || true
            kubectl -n myapp-staging logs "$BACKEND_POD" --tail=200 || true
          fi
          echo ""
          echo "=== Frontend Deployment ==="
          kubectl -n myapp-staging get deploy frontend -o wide || true
          kubectl -n myapp-staging get pods -l app=frontend -o wide || true
          FRONTEND_POD="$(kubectl -n myapp-staging get pods -l app=frontend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "$FRONTEND_POD" ]; then
            echo "=== Frontend Pod: $FRONTEND_POD ==="
            kubectl -n myapp-staging describe pod "$FRONTEND_POD" || true
            kubectl -n myapp-staging logs "$FRONTEND_POD" --tail=200 || true
          fi
          echo ""
          echo "=== Postgres StatefulSet ==="
          kubectl -n myapp-staging get statefulset postgres -o wide || true
          kubectl -n myapp-staging get pods -l app=postgres -o wide || true
