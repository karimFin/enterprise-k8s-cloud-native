name: CI/CD Pipeline

on:
  push:
    branches: ['**']
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ghcr.io/${{ github.repository_owner }}/myapp

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # =========================================================================
  # Job 1: Lint and Test
  # =========================================================================
  test:
    name: Test ${{ matrix.service }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        service: [frontend, backend]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: ${{ matrix.service }}/package-lock.json
      - name: Install dependencies
        working-directory: ${{ matrix.service }}
        run: npm ci
      - name: Run tests
        working-directory: ${{ matrix.service }}
        run: npm test

  # =========================================================================
  # Job 2: Build Docker images and push to GHCR
  # =========================================================================
  build-and-push:
    name: Build ${{ matrix.service }}
    needs: test
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [frontend, backend]
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_PREFIX }}-${{ matrix.service }}
          tags: |
            type=sha,prefix=
            type=raw,value=${{ github.sha }}
            type=ref,event=branch
            type=raw,value=latest,enable={{is_default_branch}}
            type=semver,pattern={{version}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./${{ matrix.service }}
          push: true
          platforms: linux/amd64,linux/arm64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: VITE_API_URL=

      - name: Trivy vulnerability scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.IMAGE_PREFIX }}-${{ matrix.service }}:${{ github.sha }}
          format: 'sarif'
          output: 'trivy-${{ matrix.service }}.sarif'
          severity: 'CRITICAL,HIGH'

  # =========================================================================
  # Job 3: Validate K8s manifests
  # =========================================================================
  validate-k8s:
    name: Validate K8s Manifests
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Validate staging
        run: kustomize build k8s/overlays/staging > /dev/null
      - name: Validate prod
        run: kustomize build k8s/overlays/prod > /dev/null
      - name: Install kubeconform
        run: |
          curl -sLO https://github.com/yannh/kubeconform/releases/latest/download/kubeconform-linux-amd64.tar.gz
          tar xzf kubeconform-linux-amd64.tar.gz
          sudo mv kubeconform /usr/local/bin/
      - name: Schema validation
        run: kustomize build k8s/overlays/prod | kubeconform -strict -summary -kubernetes-version 1.28.0

  # =========================================================================
  # Job 4: Deploy to Staging (automatic)
  # =========================================================================
  deploy-staging:
    name: Deploy to Staging
    needs: [build-and-push, validate-k8s]
    if: ${{ false }}
    runs-on: ubuntu-latest
    environment:
      name: staging
      url: https://staging.myapp.example.com
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Prepare kubeconfig (staging)
        id: prep-kube-staging
        shell: bash
        env:
          RAW_KUBECONFIG: ${{ secrets.KUBE_CONFIG_STAGING }}
        run: |
          if echo "$RAW_KUBECONFIG" | grep -q 'apiVersion:'; then
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            echo "$RAW_KUBECONFIG" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "$RAW_KUBECONFIG" | base64 -d > kc.yaml || echo "$RAW_KUBECONFIG" | base64 --decode > kc.yaml
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            cat kc.yaml >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV
      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ steps.prep-kube-staging.outputs.kubeconfig }}

      - name: Update image tags and deploy
        run: |
          cd k8s/overlays/staging
          kustomize edit set image \
            backend=${{ env.IMAGE_PREFIX }}-backend:${{ github.sha }} \
            frontend=${{ env.IMAGE_PREFIX }}-frontend:${{ github.sha }}
          kubectl apply -k .
          kubectl rollout status deployment/backend -n myapp-staging --timeout=300s
          kubectl rollout status deployment/frontend -n myapp-staging --timeout=300s

      - name: Smoke test
        run: |
          sleep 10
          STATUS=$(kubectl exec -n myapp-staging deploy/backend -- wget -qO- http://localhost:8080/health | grep -o '"ok"' || echo "FAIL")
          [ "$STATUS" = '"ok"' ] && echo "âœ… Staging healthy" || (echo "âŒ Failed" && exit 1)

      - name: Rollback on failure
        if: failure()
        run: |
          kubectl rollout undo deployment/backend -n myapp-staging
          kubectl rollout undo deployment/frontend -n myapp-staging

  # =========================================================================
  # Job 5: Deploy to Production (auto after Staging Kind, if kubeconfig present)
  # =========================================================================
  deploy-prod:
    name: Deploy to Production
    needs: deploy-staging-kind
    if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Prepare kubeconfig (production)
        id: prep-kube-prod
        shell: bash
        env:
          RAW_KUBECONFIG: ${{ secrets.KUBE_CONFIG_PROD }}
        run: |
          if [ -z "$RAW_KUBECONFIG" ]; then
            echo "âš ï¸  KUBE_CONFIG_PROD secret not configured. Skipping production deployment."
            echo "skip_deploy=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "skip_deploy=false" >> $GITHUB_OUTPUT
          if echo "$RAW_KUBECONFIG" | grep -q 'apiVersion:'; then
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            echo "$RAW_KUBECONFIG" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "$RAW_KUBECONFIG" | base64 -d > kc.yaml 2>/dev/null || echo "$RAW_KUBECONFIG" | base64 --decode > kc.yaml
            echo "kubeconfig<<EOF" >> $GITHUB_OUTPUT
            cat kc.yaml >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi
      - name: Skip deployment notice
        if: steps.prep-kube-prod.outputs.skip_deploy == 'true'
        run: |
          echo "â„¹ï¸  Production deployment skipped: KUBE_CONFIG_PROD secret not configured"
          echo "To enable production deployments, configure the KUBE_CONFIG_PROD secret in GitHub repository settings"
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV
      - name: Configure kubectl
        if: steps.prep-kube-prod.outputs.skip_deploy != 'true'
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ steps.prep-kube-prod.outputs.kubeconfig }}

      - name: Update image tags and deploy
        if: steps.prep-kube-prod.outputs.skip_deploy != 'true'
        run: |
          cd k8s/overlays/prod
          kustomize edit set image \
            backend=${{ env.IMAGE_PREFIX }}-backend:${{ github.sha }} \
            frontend=${{ env.IMAGE_PREFIX }}-frontend:${{ github.sha }}
          kubectl apply -k .
          kubectl rollout status deployment/backend -n myapp-production --timeout=600s
          kubectl rollout status deployment/frontend -n myapp-production --timeout=600s

      - name: Verify and rollback if unhealthy
        if: steps.prep-kube-prod.outputs.skip_deploy != 'true'
        run: |
          sleep 15
          STATUS=$(kubectl exec -n myapp-production deploy/backend -- wget -qO- http://localhost:8080/health | grep -o '"ok"' || echo "FAIL")
          if [ "$STATUS" != '"ok"' ]; then
            echo "âŒ Rolling back production..."
            kubectl rollout undo deployment/backend -n myapp-production
            kubectl rollout undo deployment/frontend -n myapp-production
            exit 1
          fi
          echo "âœ… Production deployment successful"

      - name: Get Frontend URL
        if: steps.prep-kube-prod.outputs.skip_deploy != 'true' && success()
        run: |
          echo "ðŸ“‹ Production deployment complete!"
          echo ""
          echo "Retrieving frontend URL..."
          FRONTEND_HOST=$(kubectl get ingress myapp-ingress -n myapp-production -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "")
          if [ -z "$FRONTEND_HOST" ]; then
            echo "âš ï¸  Ingress not yet provisioned. Trying service endpoint..."
            FRONTEND_IP=$(kubectl get svc frontend -n myapp-production -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
            if [ -z "$FRONTEND_IP" ]; then
              FRONTEND_IP=$(kubectl get svc frontend -n myapp-production -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            fi
            if [ -n "$FRONTEND_IP" ]; then
              echo "ðŸŒ Frontend URL: http://$FRONTEND_IP"
            else
              echo "âš ï¸  Service LoadBalancer IP/Hostname not yet assigned. Check with:"
              echo "   kubectl get svc frontend -n myapp-production"
            fi
          else
            echo "ðŸŒ Frontend URL: https://$FRONTEND_HOST"
          fi
          echo ""
          echo "Backend health: $(kubectl get svc backend -n myapp-production -o jsonpath='{.status.loadBalancer.ingress[0].ip}' || echo 'Check with kubectl get svc backend -n myapp-production')"

      - name: Rollback on failure
        if: steps.prep-kube-prod.outputs.skip_deploy != 'true' && failure()
        run: |
          kubectl rollout undo deployment/backend -n myapp-production
          kubectl rollout undo deployment/frontend -n myapp-production

  # =========================================================================
  # Job 4b: Deploy to Staging on ephemeral Kind cluster (free runner)
  # Enabled when repository variable USE_KIND_FOR_STAGING == "true"
  # =========================================================================
  deploy-staging-kind:
    name: Deploy to Staging (Kind)
    needs: [build-and-push, validate-k8s]
    if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: imranismail/setup-kustomize@v2
      - name: Normalize IMAGE_PREFIX (lowercase owner)
        shell: bash
        run: echo "IMAGE_PREFIX=ghcr.io/${GITHUB_REPOSITORY_OWNER,,}/myapp" >> $GITHUB_ENV
      - name: Create Kind cluster
        uses: helm/kind-action@v1.8.0
        with:
          cluster_name: myapp-staging
          node_image: kindest/node:v1.28.0
      - name: Build images locally and load into Kind
        run: |
          # Build backend image
          docker build -t "${IMAGE_PREFIX}-backend:${GITHUB_SHA}" ./backend
          # Build frontend image (pass build arg for completeness)
          docker build -t "${IMAGE_PREFIX}-frontend:${GITHUB_SHA}" --build-arg VITE_API_URL= ./frontend
          # Load images into the Kind cluster so Kubelet can use IfNotPresent without pulling
          kind load docker-image "${IMAGE_PREFIX}-backend:${GITHUB_SHA}" --name myapp-staging
          kind load docker-image "${IMAGE_PREFIX}-frontend:${GITHUB_SHA}" --name myapp-staging
      - name: Update image tags and deploy to Kind
        run: |
          cd k8s/overlays/staging
          kustomize edit set image \
            backend=${{ env.IMAGE_PREFIX }}-backend:${{ github.sha }} \
            frontend=${{ env.IMAGE_PREFIX }}-frontend:${{ github.sha }}
          kubectl apply -k .
          # Remove imagePullSecrets for Kind (not needed for locally loaded images)
          # Create a dummy secret to prevent pod startup failures if imagePullSecrets reference doesn't exist
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=dummy \
            --docker-password=dummy \
            --docker-email=dummy@example.com \
            -n myapp-staging \
            --dry-run=client -o yaml | kubectl apply -f - 2>/dev/null || true
          # Remove podAntiAffinity for Kind (single node, causes scheduling issues)
          kubectl patch deployment backend -n myapp-staging --type='json' -p='[{"op": "remove", "path": "/spec/template/spec/affinity"}]' 2>/dev/null || true
          kubectl patch deployment frontend -n myapp-staging --type='json' -p='[{"op": "remove", "path": "/spec/template/spec/affinity"}]' 2>/dev/null || true
          # Check initial pod status
          echo "Checking initial pod status..."
          kubectl get pods -n myapp-staging -o wide
          echo ""
          # Wait for deployments to finish rolling out before running smoke tests
          echo "Waiting for backend rollout..."
          kubectl rollout status deployment/backend -n myapp-staging --timeout=300s || {
            echo "Backend rollout failed, checking pod status..."
            kubectl get pods -n myapp-staging -l app=backend -o wide
            kubectl describe pods -n myapp-staging -l app=backend | tail -100
            kubectl get events -n myapp-staging --sort-by='.lastTimestamp' | grep -i backend | tail -20
            exit 1
          }
          echo "Waiting for frontend rollout..."
          kubectl rollout status deployment/frontend -n myapp-staging --timeout=300s || {
            echo "Frontend rollout failed, checking pod status..."
            kubectl get pods -n myapp-staging -l app=frontend -o wide
            kubectl describe pods -n myapp-staging -l app=frontend | tail -100
            kubectl get events -n myapp-staging --sort-by='.lastTimestamp' | grep -i frontend | tail -20
            exit 1
          }
          # Give pods a moment to stabilize after rollout
          echo "Waiting for pods to stabilize..."
          sleep 10

      - name: Get Staging Frontend URL
        run: |
          echo "ðŸ“‹ Staging deployment complete!"
          echo ""
          echo "Staging cluster details:"
          FRONTEND_POD=$(kubectl get pods -n myapp-staging -l app=frontend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          BACKEND_POD=$(kubectl get pods -n myapp-staging -l app=backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          
          echo "Frontend Pod: $FRONTEND_POD"
          echo "Backend Pod: $BACKEND_POD"
          echo ""
          echo "Service endpoints:"
          kubectl get svc -n myapp-staging frontend backend || true
          echo ""
          echo "Note: Staging runs on an ephemeral Kind cluster (local in GitHub Actions runner)"
          echo "This cluster is temporary and will be deleted after workflow completes."
          echo ""
          echo "To test locally if you run Kind cluster:"
          echo "  kubectl port-forward -n myapp-staging svc/frontend 3000:80"
          echo "  Then visit: http://localhost:3000"

      - name: Smoke test (Kind)
        if: ${{ false }}
        run: |
          echo "Waiting for backend pods to be ready..."
          kubectl wait --for=condition=Ready pods -l app=backend -n myapp-staging --timeout=180s || {
            echo "Backend pods not ready, checking status..."
            kubectl get pods -n myapp-staging -l app=backend
            exit 1
          }
          echo "Waiting for frontend pods to be ready..."
          kubectl wait --for=condition=Ready pods -l app=frontend -n myapp-staging --timeout=180s || {
            echo "Frontend pods not ready, checking status..."
            kubectl get pods -n myapp-staging -l app=frontend
            exit 1
          }
          BACKEND_POD=$(kubectl get pods -n myapp-staging -l app=backend -o jsonpath='{.items[0].metadata.name}')
          if [ -z "$BACKEND_POD" ]; then
            echo "âŒ No ready backend pod found"
            kubectl get pods -n myapp-staging -l app=backend
            exit 1
          fi
          echo "Using backend pod: $BACKEND_POD"
          # Give the cluster a bit of extra time and retry the health check
          ATTEMPTS=12
          SLEEP_SECONDS=5
          for i in $(seq 1 $ATTEMPTS); do
            STATUS=$(kubectl exec -n myapp-staging "$BACKEND_POD" -- wget -qO- http://localhost:8080/health 2>/dev/null | grep -o '"ok"' || echo "FAIL")
            if [ "$STATUS" = '"ok"' ]; then
              echo "âœ… Staging(kind) healthy"
              exit 0
            fi
            echo "Health check attempt $i/$ATTEMPTS failed, waiting ${SLEEP_SECONDS}s..."
            sleep "$SLEEP_SECONDS"
          done
          echo "âŒ Failed after $ATTEMPTS attempts"
          echo "Backend pod logs:"
          kubectl logs -n myapp-staging "$BACKEND_POD" --tail=50 || true
          exit 1
      - name: Debug deployment failures
        if: failure()
        run: |
          echo "=== Backend Deployment ==="
          kubectl -n myapp-staging get deploy backend -o wide || true
          kubectl -n myapp-staging get pods -l app=backend -o wide || true
          BACKEND_POD="$(kubectl -n myapp-staging get pods -l app=backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "$BACKEND_POD" ]; then
            echo "=== Backend Pod: $BACKEND_POD ==="
            kubectl -n myapp-staging describe pod "$BACKEND_POD" || true
            kubectl -n myapp-staging logs "$BACKEND_POD" --tail=200 || true
          fi
          echo ""
          echo "=== Frontend Deployment ==="
          kubectl -n myapp-staging get deploy frontend -o wide || true
          kubectl -n myapp-staging get pods -l app=frontend -o wide || true
          FRONTEND_POD="$(kubectl -n myapp-staging get pods -l app=frontend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "$FRONTEND_POD" ]; then
            echo "=== Frontend Pod: $FRONTEND_POD ==="
            kubectl -n myapp-staging describe pod "$FRONTEND_POD" || true
            kubectl -n myapp-staging logs "$FRONTEND_POD" --tail=200 || true
          fi
          echo ""
          echo "=== Postgres StatefulSet ==="
          kubectl -n myapp-staging get statefulset postgres -o wide || true
          kubectl -n myapp-staging get pods -l app=postgres -o wide || true
